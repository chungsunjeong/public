{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Dense, Lambda, Flatten, Activation, Dropout, concatenate)\n",
    "from keras.losses import mse, binary_crossentropy, categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "sys.path.append('./*')\n",
    "sys.path=list(set(sys.path))\n",
    "\n",
    "from model.keras_models import create_dense_layers, inst_layers, sampling\n",
    "from module_DTI.default_load import *\n",
    "from module_DTI.utils import *\n",
    "\n",
    "config = collections.namedtuple('config', ['Dataset'])\n",
    "dict_directories = {'dir_ROOT': './dataset/final', }\n",
    "dict_directories.update({\n",
    "    'DTI_adjmat': dict_directories['dir_ROOT'] + '/drug-target_mat.tsv',\n",
    "    'drug': dict_directories['dir_ROOT'] + '/drug_descriptor.tsv',\n",
    "    'target': dict_directories['dir_ROOT'] + '/protein_descriptor.tsv'\n",
    "})\n",
    "config_Dataset = {\n",
    "        'dict_directories': dict_directories,\n",
    "        'neg_to_pos_ratio': 1,\n",
    "        'split_ratio': 0.8,\n",
    "        'pos_filename': 'pos_sample.txt',\n",
    "        'neg_filename': 'neg_sample_1.txt',\n",
    "}\n",
    "save_path='./model/model_saved/'\n",
    "checkpoint_path='./model/model_checkpoints/'\n",
    "config.Dataset = config_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ae=keras.models.load_model(save_path+'pair_dim500_e100_b100.hdf')\n",
    "pair_layer=pair_ae.layers[1].layers[1:]\n",
    "drug_target_intput=Input(shape=(1627,),name='DT_input')\n",
    "drug_target_vector=inst_layers(pair_layer,drug_target_intput)\n",
    "M1=Model(inputs=drug_target_intput,outputs=drug_target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Load DTI data.\n",
      "--------------------------------------------------------\n",
      "Load existing positive & negative sample files.\n",
      "# of positive samples: 9592\n",
      "# of features of a positive sample: 1627\n",
      "\n",
      "# of negative samples: 9592\n",
      "# of features of a negative sample: 1627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_verbose_dataset=0\n",
    "opt_verbose_training=0\n",
    "opt_loss_loc=0\n",
    "opt_acc_loc=1\n",
    "DTI=load_DTI(config,verbose=opt_verbose_dataset)\n",
    "load_pos_neg_samples(config,DTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/model_checkpoints/tot_SVM_stack_10sets_5cv\n"
     ]
    }
   ],
   "source": [
    "N_label='tot'\n",
    "name_model='SVM_stack' \n",
    "name_test='10sets_5cv'\n",
    "opt_save_model='on'\n",
    "neg_file_list=['0','1','2']#,'3','4']#,'5','6','7','8','9']\n",
    "epochs=100\n",
    "label_lr=0.001\n",
    "l_model_filename=checkpoint_path+'_'.join([N_label,name_model,name_test])\n",
    "print(l_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: SVM_stack\n",
      "N_label: tot\n",
      "-----------------------------------------------\n",
      "neg_sample_0.txt\n",
      "Load existing positive & negative sample files.\n",
      "1 cross-validation\n",
      "2 cross-validation\n",
      "3 cross-validation\n",
      "4 cross-validation\n",
      "5 cross-validation\n",
      "-----------------------------------------------\n",
      "neg_sample_1.txt\n",
      "Load existing positive & negative sample files.\n",
      "1 cross-validation\n",
      "2 cross-validation\n",
      "3 cross-validation\n",
      "4 cross-validation\n",
      "5 cross-validation\n",
      "-----------------------------------------------\n",
      "neg_sample_2.txt\n",
      "Load existing positive & negative sample files.\n",
      "1 cross-validation\n",
      "2 cross-validation\n",
      "3 cross-validation\n",
      "4 cross-validation\n",
      "5 cross-validation\n",
      "--------------------finish---------------------------\n"
     ]
    }
   ],
   "source": [
    "acc_list,auroc_list,TPR_list,TNR_list,PRE_list,F1_list=[],[],[],[],[],[]\n",
    "print('MODEL: '+name_model)\n",
    "print('N_label: '+str(N_label))\n",
    "for neg in neg_file_list: \n",
    "    print('-----------------------------------------------')   \n",
    "    config.Dataset['neg_filename']='neg_sample_'+neg+'.txt'\n",
    "    print(config.Dataset['neg_filename'])\n",
    "    load_pos_neg_samples(config,DTI,verbose=0)\n",
    "    \n",
    "    rand_ind=np.arange(9592)\n",
    "    np.random.shuffle(rand_ind)\n",
    "    for order_CV in range(1,6):\n",
    "        print(str(order_CV)+' cross-validation')\n",
    "        train_x,train_y,test_x,test_y=load_train_test_5fold_CV(config,DTI,rand_ind,order_CV)\n",
    "#         train_x,train_y,test_x,test_y=load_train_test(config,DTI,verbose=opt_verbose_dataset)\n",
    "        label_train_x,label_train_y = load_label_train_test(train_x,train_y,N_label=N_label,\n",
    "                                                            stacked_model=M1)\n",
    "        label_test_x=M1.predict(test_x)\n",
    "        label_test_y=test_y\n",
    "        input_dim=int(np.shape(label_train_x)[1])\n",
    "\n",
    "        classifier= svm.SVC(gamma='scale',probability=True)\n",
    "        classifier.fit(label_train_x,np.argmax(label_train_y, axis=-1))\n",
    "        y_pred,accuracy,auroc,TPR,TNR,PRE,F1=get_metrics_values(classifier, label_test_x, label_test_y, SVM=True,verbose=0)\n",
    "        \n",
    "        acc_list.append(accuracy)\n",
    "        auroc_list.append(auroc)\n",
    "        TPR_list.append(TPR)\n",
    "        TNR_list.append(TNR)\n",
    "        PRE_list.append(PRE)\n",
    "        F1_list.append(F1)\n",
    "        \n",
    "#         plot_roc_curve(classifier,label_test_x,label_test_y,SVM=True) \n",
    "#         print(l_model_filename)\n",
    "#         print(accuracy)\n",
    "#         print(F1)\n",
    "\n",
    "with open(l_model_filename+'.txt','w') as f:\n",
    "    acc=[float(v) for v in acc_list]\n",
    "    auroc=[float(v) for v in auroc_list]\n",
    "    f1=[float(v) for v in F1_list]\n",
    "    f.write('Avg acc:\\n' + str(np.average(acc)))\n",
    "    f.write('\\nStd acc:\\n' + str(np.std(acc)))\n",
    "    f.write('\\nAvg auroc:\\n' + str(np.average(auroc)))\n",
    "    f.write('\\nStd auroc:\\n' + str(np.std(auroc)))\n",
    "    f.write('\\nAvg F1:\\n' + str(np.average(f1)))\n",
    "    f.write('\\nStd acc:\\n' + str(np.std(f1)))\n",
    "    f.write('\\nacc\\n'+'\\n'.join(acc_list))\n",
    "    f.write('\\nauroc\\n'+'\\n'.join(auroc_list))\n",
    "    f.write('\\nTPR\\n'+'\\n'.join(TPR_list))\n",
    "    f.write('\\nTNR\\n'+'\\n'.join(TNR_list))\n",
    "    f.write('\\nPRE\\n'+'\\n'.join(PRE_list))\n",
    "    f.write('\\nF1\\n'+'\\n'.join(F1_list))\n",
    "print('--------------------finish---------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y_test=[]\n",
    "# for v in label_test_y:\n",
    "#     y_test.append(list(v).index(1))\n",
    "# y_pred = classifier.predict(label_test_x)[:, 1]\n",
    "# fpr, tpr, threshold = roc_curve(y_test, y_pred, pos_label=1)\n",
    "# with open('./model/model_checkpoints/g_SVM_tpr_normal.txt','w') as f:\n",
    "#     kk=[str(v) for v in list(tpr)]\n",
    "#     f.write('\\t'.join(list(kk)))\n",
    "# roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
