{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from load_dataset import get_preprocessed_dataset, load_json_file\n",
    "import tensorflow as tf\n",
    "from tensorflow import identity\n",
    "from tensorflow.nn import conv2d, dropout, relu, sigmoid, softmax, bidirectional_dynamic_rnn, softmax_cross_entropy_with_logits\n",
    "from tensorflow.nn.rnn_cell import LSTMCell,DropoutWrapper\n",
    "from main import get_batch_dict, get_init_charEmbVec\n",
    "from layer import CharEmbLayer,WordEmbLayer,DenseLayer,HighwayLayer,\\\n",
    "    ContextualEmbLayer,AttentionLayer,TwoLSTMs_ModelingLayer,OutputLayer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "path = 'D:/Wisdom/git/tmp/BIDAF/data/squad/'\n",
    "version = 'small'\n",
    "json_file = path + mode + '-' + version + '.json'\n",
    "data = load_json_file(json_file)\n",
    "config, full_dataset, sub_info_dataset, index_dataset, word2vec_dataset = \\\n",
    "    get_preprocessed_dataset(data, mode='train')\n",
    "is_training=True\n",
    "\n",
    "n_sample = len(full_dataset['q_dataset'])\n",
    "max_word_num_x = config['max_word_num_x']\n",
    "max_word_num_q = config['max_word_num_q']\n",
    "max_word_len = config['max_word_len']\n",
    "word_vec_dim = config['word_vec_dim']\n",
    "d = 175\n",
    "config['hidden_size'] = d\n",
    "batch_size = 30\n",
    "config['batch_size']=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vec_dim=10\n",
    "tot_char_list=sub_info_dataset['tot_char_list']\n",
    "config['char_vec_dim']=char_vec_dim\n",
    "char2vec_dataset=get_init_charEmbVec(index_dataset,tot_char_list,char_vec_dim,initializer=np.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "xw = tf.placeholder(dtype='float32', shape=[batch_size, max_word_num_x, word_vec_dim], name='x_word')\n",
    "xc = tf.placeholder(dtype='float32', shape=[batch_size, max_word_num_x, max_word_len, char_vec_dim], name='x_char')\n",
    "qw = tf.placeholder(dtype='float32', shape=[batch_size, max_word_num_q, word_vec_dim], name='q_word')\n",
    "qc = tf.placeholder(dtype='float32', shape=[batch_size, max_word_num_q, max_word_len, char_vec_dim], name='q_char')\n",
    "len_x = tf.placeholder(dtype='int32', shape=[batch_size], name='word_len_x')\n",
    "len_q = tf.placeholder(dtype='int32', shape=[batch_size], name='word_len_q')\n",
    "y1 = tf.placeholder(dtype='bool', shape=[batch_size,max_word_num_x], name='y_start')\n",
    "y2 = tf.placeholder(dtype='bool', shape=[batch_size,max_word_num_x], name='y_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_init=tf.truncated_normal_initializer(stddev=0.5)\n",
    "weight_init=tf.truncated_normal_initializer(stddev=0.5)\n",
    "bias_init=tf.truncated_normal_initializer(stddev=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel_num = char_vec_dim\n",
    "output_channel_num = 100\n",
    "height = 5\n",
    "filter_shape = [1, height, input_channel_num, output_channel_num]\n",
    "stride = [1, 1, 1, 1]\n",
    "padding = 'VALID'\n",
    "cnn_xc = CharEmbLayer('CNN', xc, filter_shape, output_channel_num, stride, padding, cnn_init, is_training=is_training)\n",
    "cnn_qc = CharEmbLayer('CNN', qc, filter_shape, output_channel_num, stride, padding, cnn_init, reuse=True, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x = WordEmbLayer('WordEmb_x', xw, cnn_xc)\n",
    "emb_q = WordEmbLayer('WordEmb_q', qw, cnn_qc)\n",
    "\n",
    "highway_x = HighwayLayer('Highway', emb_x, weight_init, bias_init, is_training=is_training)\n",
    "highway_q = HighwayLayer('Highway', emb_q, weight_init, bias_init, reuse=True, is_training=is_training)\n",
    "\n",
    "h = ContextualEmbLayer('ContEmb', highway_x, d, len_x, is_training=is_training)\n",
    "u = ContextualEmbLayer('ContEmb', highway_q, d, len_q, reuse=True, is_training=is_training)\n",
    "\n",
    "G = AttentionLayer('Attention', [h, u], d, weight_init, is_training=is_training)\n",
    "\n",
    "M = TwoLSTMs_ModelingLayer('Modeling', G, d, len_x, is_training=is_training)\n",
    "logit1,logit2,p1,p2=OutputLayer('Output', [G,M], d, len_x, weight_init, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1=tf.reduce_mean(softmax_cross_entropy_with_logits(labels=y1,logits=logit1))\n",
    "loss2=tf.reduce_mean(softmax_cross_entropy_with_logits(labels=y2,logits=logit2))\n",
    "loss = loss1 + loss2\n",
    "optimizer=tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch_dict):\n",
    "    feed={xw:batch_dict['xw'],\n",
    "     xc:batch_dict['xc'],\n",
    "     qw:batch_dict['qw'],\n",
    "     qc:batch_dict['qc'],\n",
    "     len_x:batch_dict['x_len'],\n",
    "     len_q:batch_dict['q_len'],\n",
    "     y1:batch_dict['y_start'],\n",
    "     y2:batch_dict['y_end']}\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config=tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# sess=tf.Session(config=config)\n",
    "sess=tf.Session()\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 30\n",
      "-------- Start --------\n",
      "1/109 complete.\tLoss: 18.9098\tTime: 17.1553[s]\n",
      "2/109 complete.\tLoss: 14.5220\tTime: 16.0002[s]\n",
      "3/109 complete.\tLoss: 13.5909\tTime: 19.3116[s]\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ' + str(batch_size))\n",
    "print('-------- Start --------')\n",
    "sample_index = np.arange(n_sample)\n",
    "np.random.shuffle(sample_index)\n",
    "batch_time = n_sample // batch_size\n",
    "\n",
    "\n",
    "for i in range(batch_time):\n",
    "    t_start=time.time()\n",
    "    index_range =  sample_index[i * batch_size:(i+1) * batch_size]\n",
    "    feed=get_feed_dict(get_batch_dict(index_range, config, full_dataset, word2vec_dataset, char2vec_dataset))\n",
    "    l,_=sess.run([loss,optimizer],feed_dict=feed)\n",
    "    t_end=time.time()\n",
    "    if i==3:break\n",
    "    if i%1 == 0 : print('%d/%d complete.\\tLoss: %.4f\\tTime: %.4f[s]' %(i+1,batch_time,l,t_end-t_start))\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
