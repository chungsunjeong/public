{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import namedtuple, defaultdict\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str_lower_cleaner(s):\n",
    "    return s.replace(\"''\",'\" ').replace('``','\" ').lower()\n",
    "\n",
    "def my_tokenize(s):\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = ['\"' if t == \"''\" or t=='``' else t for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def load_json_file(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def get_corresponding_glove_word2vec(filename,word_list):\n",
    "    word_vec_dim = int(filename.split('.')[-2].replace('d', ''))\n",
    "    word2vec = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line[:line.index(' ')]\n",
    "            if word in word_list:\n",
    "                l = line.rstrip().split(\" \")\n",
    "                word2vec[word] = list(map(float, l[1:]))\n",
    "    return word2vec,word_vec_dim\n",
    "# old_word2vec_word_list=set(word2vec.keys())\n",
    "# new_word2vec_word_list=tot_word_list.difference(old_word2vec_word_list)\n",
    "# print(len(tot_word_list.difference(old_word2vec_word_list)))\n",
    "# print(len(old_word2vec_word_list.difference(tot_word_list)))\n",
    "# # word_list contain(>) word2vec_word_list\n",
    "\n",
    "def get_word_char_counter(data):\n",
    "    context_word_counter = defaultdict(int)\n",
    "    context_char_counter = defaultdict(int)\n",
    "    query_word_counter = defaultdict(int)\n",
    "    query_char_counter = defaultdict(int)\n",
    "    n_article = len(data['data'])\n",
    "    for a in range(n_article):\n",
    "        for c in range(len(data['data'][a]['paragraphs'])):\n",
    "            context = str_lower_cleaner(data['data'][a]['paragraphs'][c]['context'])\n",
    "            for char in set(context):\n",
    "                context_char_counter[char] += context.count(char)\n",
    "            word_list = my_tokenize(context)\n",
    "\n",
    "            for word in word_list:\n",
    "                context_word_counter[word] += 1\n",
    "\n",
    "            for q in range(len(data['data'][a]['paragraphs'][c]['qas'])):\n",
    "                query = str_lower_cleaner(data['data'][a]['paragraphs'][c]['qas'][q]['question'])\n",
    "                for char in set(query):\n",
    "                    query_char_counter[char] += query.count(char)\n",
    "                word_list = my_tokenize(query)\n",
    "                for w in word_list:\n",
    "                    query_word_counter[w] += 1\n",
    "    return context_word_counter,context_char_counter,query_word_counter,query_char_counter\n",
    "\n",
    "def get_ind_dictionaries(word_list,char_list):\n",
    "    word2ind, ind2word = dict(), dict()\n",
    "    char2ind, ind2char = dict(), dict()\n",
    "    for i, key in enumerate(word_list):\n",
    "        word2ind[key] = i + 1\n",
    "        ind2word[i + 1] = key\n",
    "\n",
    "    for i, key in enumerate(char_list):\n",
    "        char2ind[key] = i + 1\n",
    "        ind2char[i + 1] = key\n",
    "\n",
    "    word2ind['-EMPTY-'] = 0\n",
    "    ind2word[0] = '-EMPTY-'\n",
    "    char2ind['-EMPTY-'] = 0\n",
    "    ind2char[0] = '-EMPTY-'\n",
    "    return word2ind, ind2word, char2ind, ind2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d='D:/Wisdom/git/tmp/BIDAF/data/squad/'+mode+'-v1.1.json'\n",
    "d='D:/Wisdom/git/tmp/BIDAF/data/squad/'+mode+'-small.json'\n",
    "with open(d,'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=defaultdict(dict)\n",
    "full_dataset=defaultdict(dict)\n",
    "sub_info_dataset=defaultdict(dict)\n",
    "counter_dataset=defaultdict(dict)\n",
    "index_dataset=defaultdict(dict)\n",
    "word2vec_dataset=defaultdict(dict)\n",
    "\n",
    "max_word_num_x = 0\n",
    "max_word_num_q = 0\n",
    "max_word_len = 0\n",
    "\n",
    "context_word_counter, context_char_counter, query_word_counter, query_char_counter=get_word_char_counter(data)\n",
    "\n",
    "tot_word_list = set(query_word_counter.keys()).union(set(context_word_counter.keys()))\n",
    "tot_char_list = set(query_char_counter.keys()).union(set(context_char_counter.keys()))\n",
    "\n",
    "word2ind, ind2word, char2ind, ind2char = get_ind_dictionaries(tot_word_list, tot_char_list)\n",
    "\n",
    "glove_file = 'D:/Wisdom/git/tmp/BIDAF/data/glove/glove.840B.300d.txt'\n",
    "word2vec, word_vec_dim=get_corresponding_glove_word2vec(glove_file, tot_word_list)\n",
    "old_word2vec_word_list = set(word2vec.keys())\n",
    "new_word2vec_word_list = tot_word_list.difference(old_word2vec_word_list)\n",
    "# Initialization of new word's embedding vector\n",
    "new_word_vec_init = list(map(list, np.random.normal(0, 0.5, size=[len(new_word2vec_word_list), word_vec_dim])))\n",
    "for i, word in enumerate(new_word2vec_word_list):\n",
    "    word2vec[word] = new_word_vec_init[i]\n",
    "\n",
    "word2char = dict()\n",
    "ind2vec = dict()\n",
    "\n",
    "for word in tot_word_list:\n",
    "    ind2vec[word2ind[word]] = word2vec[word]\n",
    "    word2char[word] = list(word)\n",
    "    max_word_len = max(max_word_len, len(word))\n",
    "\n",
    "x_sample_ind = 0\n",
    "q_sample_ind = 0\n",
    "ans_sample_ind = 0\n",
    "n_article = len(data['data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {key=x_sample_ind, value=[a context's words' indexes list (shape: [max_word_num_x])]} \n",
    "x_dataset=dict()\n",
    "# {key=x_sample_ind, value=[a context's wodrds' chars' indexes list (shape: [max_word_num_x by max_word_len])]} \n",
    "x_char_dataset=dict()\n",
    "# {key=q_sample_ind, value=[a query's words' indexes list (shape: [max_word_num_q])]} \n",
    "q_dataset=dict()\n",
    "# {key=q_sample_ind, value=[a query's words' chars' indexes list (shape: [max_word_num_q by max_word_len])]} \n",
    "q_char_dataset=dict()\n",
    "# {key=q_sample_ind, value=[answers' start_word's index (shape: [answer_num by 1])]} \n",
    "y_start_dataset=dict()\n",
    "# {key=q_sample_ind, value=[answers' end_word's index (shape: [answer_num by 1])]} \n",
    "y_end_dataset=dict()\n",
    "# {key=q_sample_ind, value=x_sample_ind } \n",
    "qind2xind=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(n_article):\n",
    "    for c in range(len(data['data'][a]['paragraphs'])):\n",
    "        word_ind_list = []\n",
    "        char_ind_list = []\n",
    "        context = str_lower_cleaner(data['data'][a]['paragraphs'][c]['context'])\n",
    "        word_list = my_tokenize(context)\n",
    "        max_word_num_x=max(max_word_num_x,len(word_list))\n",
    "        for word in word_list:\n",
    "            word_ind_list.append(word2ind[word])\n",
    "            char_ind_list.append([char2ind[char] for char in word2char[word]])\n",
    "        x_dataset[x_sample_ind] = word_ind_list\n",
    "        x_char_dataset[x_sample_ind] = char_ind_list\n",
    "        \n",
    "        for q in range(len(data['data'][a]['paragraphs'][c]['qas'])):\n",
    "            word_ind_list = []\n",
    "            char_ind_list = []\n",
    "            query = str_lower_cleaner(data['data'][a]['paragraphs'][c]['qas'][q]['question'])\n",
    "            word_list = my_tokenize(query)\n",
    "            max_word_num_q=max(max_word_num_q,len(word_list))\n",
    "            for word in word_list:\n",
    "                word_ind_list.append(word2ind[word])\n",
    "                char_ind_list.append([char2ind[char] for char in word2char[word]])\n",
    "            q_dataset[q_sample_ind] = word_ind_list\n",
    "            q_char_dataset[q_sample_ind] = char_ind_list\n",
    "            qind2xind[q_sample_ind] = x_sample_ind\n",
    "            if mode == 'train':\n",
    "                answer = data['data'][a]['paragraphs'][c]['qas'][q]['answers']\n",
    "                start = answer[0]['answer_start']\n",
    "                end = start + len(answer[0]['text'])\n",
    "                y_start_dataset[q_sample_ind] = start\n",
    "                y_end_dataset[q_sample_ind] = end\n",
    "            else:\n",
    "                answers = data['data'][a]['paragraphs'][c]['qas'][q]['answers']\n",
    "                start_list = []\n",
    "                end_list = []\n",
    "                for ans in range(len(answers)):\n",
    "                    start = answers[ans]['answer_start']\n",
    "                    end = start + len(answers[ans]['text'])\n",
    "                    start_list.append(start)\n",
    "                    end_list.append(end)\n",
    "                y_start_dataset[q_sample_ind] = start_list\n",
    "                y_end_dataset[q_sample_ind] = end_list\n",
    "            q_sample_ind += 1\n",
    "        x_sample_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_num_q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
