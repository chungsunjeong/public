{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-6406dd46ab18>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "print(mnist.train.num_examples)\n",
    "print(mnist.validation.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2750\n"
     ]
    }
   ],
   "source": [
    "# parameter and hyperparameter setting\n",
    "pixel=28\n",
    "image_size=pixel*pixel\n",
    "n_class=10\n",
    "n_hidden1=500\n",
    "n_hidden2=500\n",
    "n_latent=20\n",
    "batch_size=100\n",
    "training_epochs=5\n",
    "itr=int(mnist.train.num_examples*training_epochs/batch_size)\n",
    "print(itr)\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "display_step=1\n",
    "example_to_show=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder('float32',[None,image_size])\n",
    "keep_prob=tf.placeholder('float32')\n",
    "z_in=tf.placeholder('float32',[None,n_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "#     init=tf.Variable(tf.random_normal(shape=shape,dtype='float32'))\n",
    "    init=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def bias_variable(shape):\n",
    "#     init=tf.Variable(tf.random_normal(shape=shape,dtype='float32'))\n",
    "    init=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def fc_layer(inputs,n_outputs,activation_fn=tf.nn.relu,keep_prob=1):\n",
    "    n_inputs=int(inputs.shape[1])\n",
    "    W_fc=weight_variable([n_inputs,n_outputs])\n",
    "    b_fc=bias_variable([n_outputs])\n",
    "    logit_fc=tf.matmul(inputs,W_fc)+b_fc\n",
    "    _a_fc=activation_fn(logit_fc)\n",
    "    a_fc=tf.nn.dropout(_a_fc,keep_prob=keep_prob)\n",
    "    return a_fc,logit_fc\n",
    "\n",
    "def encoder(inputs,n_hidden1,n_hidden2,n_latent,keep_prob=1):\n",
    "    fc_layer1,_=fc_layer(inputs,n_hidden1,activation_fn=tf.nn.sigmoid,keep_prob=keep_prob)\n",
    "    fc_layer2,_=fc_layer(fc_layer1,n_hidden2,activation_fn=tf.nn.sigmoid,keep_prob=keep_prob)\n",
    "    \n",
    "    w_encoder=weight_variable([n_hidden2,n_latent*2])\n",
    "    b_encoder=bias_variable([n_latent*2])\n",
    "    gaussian_params=tf.matmul(fc_layer2,w_encoder)+b_encoder\n",
    "    \n",
    "    mean=gaussian_params[:,:n_latent]\n",
    "    # to make stddev be positive, add softplus \n",
    "    # to make stddev be stable, add a small epsilon\n",
    "    stddev=1e-6+tf.nn.softplus(gaussian_params[:,n_latent:])\n",
    "    return mean,stddev\n",
    "\n",
    "def decoder(z,n_hidden2,n_hidden1,n_outputs,keep_prob=1):\n",
    "    fc_layer1,_=fc_layer(z,n_hidden2,activation_fn=tf.nn.sigmoid,keep_prob=keep_prob)\n",
    "    fc_layer2,_=fc_layer(fc_layer1,n_hidden1,activation_fn=tf.nn.sigmoid,keep_prob=keep_prob)\n",
    "    \n",
    "    w_decoder=weight_variable([n_hidden1,n_outputs])\n",
    "    b_decoder=bias_variable([n_outputs])\n",
    "    outputs=tf.sigmoid(tf.matmul(fc_layer2,w_decoder)+b_decoder)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def variational_autoencoder(inputs,n_hidden1,n_hidden2,n_latent,keep_prob=1):\n",
    "    # encoder\n",
    "    mu,std=encoder(inputs,n_hidden1,n_hidden2,n_latent,keep_prob)\n",
    "    \n",
    "    # sampling by re-parameterization technique\n",
    "    latent_variable=mu+std*tf.random_normal(tf.shape(mu),mean=0,stddev=1,dtype='float32')\n",
    "    \n",
    "    # decoder\n",
    "    n_outputs=int(inputs.shape[1])\n",
    "    x_=decoder(latent_variable,n_hidden2,n_hidden1,n_outputs,keep_prob)\n",
    "    x_=tf.clip_by_value(x_,1e-8,1-1e-8)\n",
    "    \n",
    "    # loss\n",
    "    marginal_likelihood=tf.reduce_sum(inputs*tf.log(x_)+(1-inputs)*tf.log(1-x_),1)\n",
    "    KL_divergence=0.5*tf.reduce_sum(tf.square(mu)+tf.square(std)-tf.log(1e-7+tf.square(std))-1,1)\n",
    "    \n",
    "    marginal_likelihood=tf.reduce_mean(marginal_likelihood)\n",
    "    KL_divergence=tf.reduce_mean(KL_divergence)\n",
    "    \n",
    "    ELBO=marginal_likelihood-KL_divergence\n",
    "    \n",
    "    loss=-ELBO\n",
    "    \n",
    "    return x_,latent_variable,loss,-marginal_likelihood,KL_divergence,mu,std\n",
    "\n",
    "def generator(z,n_hidden2,n_hidden1,n_outputs):\n",
    "    return decoder(z,n_hidden2,n_hidden1,n_outputs,keep_prob=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_,z,loss,neg_marginal_likelihood,KL_divergence,mu,std=variational_autoencoder(\n",
    "    x,n_hidden1,n_hidden2,n_latent,keep_prob=keep_prob)\n",
    "optimizer=tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itreation: 0: L_tot 727.12 L_likelihood 693.45 L_divergence 33.67\n",
      "itreation: 100: L_tot 216.00 L_likelihood 215.59 L_divergence 0.41\n",
      "itreation: 200: L_tot 212.66 L_likelihood 211.88 L_divergence 0.78\n",
      "itreation: 300: L_tot 191.13 L_likelihood 187.77 L_divergence 3.37\n",
      "itreation: 400: L_tot 193.21 L_likelihood 189.90 L_divergence 3.30\n",
      "itreation: 500: L_tot 184.03 L_likelihood 179.63 L_divergence 4.40\n",
      "itreation: 600: L_tot 175.89 L_likelihood 170.72 L_divergence 5.17\n",
      "itreation: 700: L_tot 174.96 L_likelihood 169.51 L_divergence 5.45\n",
      "itreation: 800: L_tot 172.89 L_likelihood 167.24 L_divergence 5.65\n",
      "itreation: 900: L_tot 177.47 L_likelihood 169.97 L_divergence 7.50\n",
      "itreation: 1000: L_tot 157.73 L_likelihood 149.45 L_divergence 8.28\n",
      "itreation: 1100: L_tot 154.21 L_likelihood 145.10 L_divergence 9.11\n",
      "itreation: 1200: L_tot 155.52 L_likelihood 146.01 L_divergence 9.51\n",
      "itreation: 1300: L_tot 145.64 L_likelihood 136.39 L_divergence 9.25\n",
      "itreation: 1400: L_tot 140.58 L_likelihood 131.24 L_divergence 9.34\n",
      "itreation: 1500: L_tot 141.16 L_likelihood 131.19 L_divergence 9.97\n",
      "itreation: 1600: L_tot 141.44 L_likelihood 130.78 L_divergence 10.67\n",
      "itreation: 1700: L_tot 137.40 L_likelihood 126.69 L_divergence 10.71\n",
      "itreation: 1800: L_tot 136.85 L_likelihood 126.53 L_divergence 10.32\n",
      "itreation: 1900: L_tot 136.33 L_likelihood 126.24 L_divergence 10.09\n",
      "itreation: 2000: L_tot 130.03 L_likelihood 119.10 L_divergence 10.94\n",
      "itreation: 2100: L_tot 134.48 L_likelihood 123.95 L_divergence 10.52\n",
      "itreation: 2200: L_tot 133.32 L_likelihood 122.05 L_divergence 11.27\n",
      "itreation: 2300: L_tot 138.32 L_likelihood 127.15 L_divergence 11.17\n",
      "itreation: 2400: L_tot 135.67 L_likelihood 124.72 L_divergence 10.95\n",
      "itreation: 2500: L_tot 133.75 L_likelihood 122.28 L_divergence 11.47\n",
      "itreation: 2600: L_tot 136.22 L_likelihood 124.87 L_divergence 11.35\n",
      "itreation: 2700: L_tot 128.34 L_likelihood 116.86 L_divergence 11.49\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session() \n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init,feed_dict={keep_prob:0.9})\n",
    "training_epochs=10\n",
    "mnist.train.next_batch(100)\n",
    "for itrr in range(itr):\n",
    "    batch_xs,batch_ys=mnist.train.next_batch(100)\n",
    "\n",
    "    _,tot_loss,loss_likelihood,loss_divergence=sess.run(\n",
    "        (optimizer,loss,neg_marginal_likelihood,KL_divergence),\n",
    "    feed_dict={x:batch_xs,keep_prob:0.9})\n",
    "    if itrr%100==0:\n",
    "        print(\"itreation: %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f\" % (\n",
    "            itrr+1,tot_loss,loss_likelihood,loss_divergence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
