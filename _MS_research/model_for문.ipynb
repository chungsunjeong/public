{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Dense, Lambda, Flatten, Activation, Dropout, concatenate)\n",
    "from keras.losses import mse, binary_crossentropy, categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "sys.path.append('./*')\n",
    "sys.path=list(set(sys.path))\n",
    "\n",
    "from model.keras_models import create_dense_layers, inst_layers, sampling\n",
    "from module_DTI.default_load import *\n",
    "from module_DTI.utils import *\n",
    "\n",
    "config = collections.namedtuple('config', ['Dataset'])\n",
    "dict_directories = {'dir_ROOT': './dataset/final', }\n",
    "dict_directories.update({\n",
    "    'DTI_adjmat': dict_directories['dir_ROOT'] + '/drug-target_mat.tsv',\n",
    "    'drug': dict_directories['dir_ROOT'] + '/drug_descriptor.tsv',\n",
    "    'target': dict_directories['dir_ROOT'] + '/protein_descriptor.tsv'\n",
    "})\n",
    "config_Dataset = {\n",
    "        'dict_directories': dict_directories,\n",
    "        'neg_to_pos_ratio': 1,\n",
    "        'split_ratio': 0.8,\n",
    "        'pos_filename': 'pos_sample.txt',\n",
    "        'neg_filename': 'neg_sample_1.txt',\n",
    "}\n",
    "save_path='./model/model_saved/'\n",
    "checkpoint_path='./model/model_checkpoints/'\n",
    "config.Dataset = config_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ae=keras.models.load_model(save_path+'pair_dim500_final.hdf')\n",
    "pair_layer=pair_ae.layers[1].layers[1:]\n",
    "drug_target_intput=Input(shape=(1627,),name='DT_input')\n",
    "drug_target_vector=inst_layers(pair_layer,drug_target_intput)\n",
    "M1=Model(inputs=drug_target_intput,outputs=drug_target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_self_training(classifier,x,y,unlabel_data,unlabel_index,unlabel_N,top_pos,top_neg,repmat_pos,repmat_neg):\n",
    "    np.random.shuffle(unlabel_index)\n",
    "    u_pred = classifier.predict(unlabel_data[unlabel_index[:unlabel_N]])\n",
    "    top_ind=np.transpose(u_pred)[0].argsort()\n",
    "    pos_top_ind=top_ind[:top_pos]\n",
    "    neg_top_ind=top_ind[-top_neg:]\n",
    "    ind_pos_neg=np.concatenate((pos_top_ind,neg_top_ind))\n",
    "    semi_train_x=np.append(x,unlabel_data[ind_pos_neg],axis=0)\n",
    "    semi_train_y=np.append(y,np.concatenate((repmat_pos,repmat_neg)),axis=0)\n",
    "    return semi_train_x,semi_train_y,unlabel_data,unlabel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Load DTI data.\n",
      "--------------------------------------------------------\n",
      "Load existing positive & negative sample files.\n",
      "# of positive samples: 9592\n",
      "# of features of a positive sample: 1627\n",
      "\n",
      "# of negative samples: 9592\n",
      "# of features of a negative sample: 1627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_verbose_dataset=0\n",
    "opt_verbose_training=0\n",
    "opt_loss_loc=0\n",
    "opt_acc_loc=1\n",
    "DTI=load_DTI(config,verbose=opt_verbose_dataset)\n",
    "load_pos_neg_samples(config,DTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_label='tot'\n",
    "name_model='m2' \n",
    "name_test=''\n",
    "opt_save_model='on'\n",
    "neg_file_list=['0','1','2','3','4']#,'5','6','7','8','9']\n",
    "epochs=100\n",
    "label_lr=0.001\n",
    "batch_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing unlabel sample file.\n",
      "Complete loading all default dataset & variables.\n",
      "\n",
      "--------------------------------------------------------\n",
      "./model/model_checkpoints/tot_m2_semi_1000_400_\n",
      "MODEL: m2\n",
      "Semi-supervised mode: on\n",
      "N_label: tot\n",
      "-----------------------------------------------\n",
      "neg_sample_0.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 161.5625\n",
      "Accuracy: 78.7129\n",
      "AUROC: 0.8455\n",
      "TPR: 0.7738405419489317\n",
      "TNR: 0.8004168837936425\n",
      "PRE: 0.7949678800856531\n",
      "F1: 0.7842619487721152\n",
      "-----------------------------------------------\n",
      "neg_sample_1.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 164.4753\n",
      "Accuracy: 78.2178\n",
      "AUROC: 0.8445\n",
      "TPR: 0.7754038561750912\n",
      "TNR: 0.7889525794684732\n",
      "PRE: 0.786053882725832\n",
      "F1: 0.7806925498426023\n",
      "-----------------------------------------------\n",
      "neg_sample_2.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 168.5287\n",
      "Accuracy: 78.8171\n",
      "AUROC: 0.8559\n",
      "TPR: 0.7691505992704534\n",
      "TNR: 0.8071912454403335\n",
      "PRE: 0.7995666305525461\n",
      "F1: 0.7840637450199204\n",
      "-----------------------------------------------\n",
      "neg_sample_3.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 172.8516\n",
      "Accuracy: 78.7129\n",
      "AUROC: 0.85\n",
      "TPR: 0.7873892652423137\n",
      "TNR: 0.7868681605002605\n",
      "PRE: 0.7869791666666667\n",
      "F1: 0.7871841625423287\n",
      "-----------------------------------------------\n",
      "neg_sample_4.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 175.8792\n",
      "Accuracy: 78.5044\n",
      "AUROC: 0.8517\n",
      "TPR: 0.784783741532048\n",
      "TNR: 0.7853048462741011\n",
      "PRE: 0.7851929092805006\n",
      "F1: 0.7849882720875684\n",
      "(15346, 500)\n",
      "(16746, 500)\n",
      "--------------------finish---------------------------\n",
      "./model/model_checkpoints/tot_m2_semi_1000_600_\n",
      "MODEL: m2\n",
      "Semi-supervised mode: on\n",
      "N_label: tot\n",
      "-----------------------------------------------\n",
      "neg_sample_0.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 180.8163\n",
      "Accuracy: 78.7389\n",
      "AUROC: 0.846\n",
      "TPR: 0.7701928087545596\n",
      "TNR: 0.8045857217300677\n",
      "PRE: 0.7976254722072316\n",
      "F1: 0.7836691410392365\n",
      "-----------------------------------------------\n",
      "neg_sample_1.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 183.0505\n",
      "Accuracy: 80.4586\n",
      "AUROC: 0.8706\n",
      "TPR: 0.8144867118290776\n",
      "TNR: 0.7946847316310578\n",
      "PRE: 0.7986714358712315\n",
      "F1: 0.806501547987616\n",
      "-----------------------------------------------\n",
      "neg_sample_2.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 184.0602\n",
      "Accuracy: 78.0354\n",
      "AUROC: 0.8457\n",
      "TPR: 0.7910369984366857\n",
      "TNR: 0.7696717040125065\n",
      "PRE: 0.7744897959183673\n",
      "F1: 0.782675947409126\n",
      "-----------------------------------------------\n",
      "neg_sample_3.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 187.8600\n",
      "Accuracy: 78.9995\n",
      "AUROC: 0.8481\n",
      "TPR: 0.791558103178739\n",
      "TNR: 0.7884314747264201\n",
      "PRE: 0.7890909090909091\n",
      "F1: 0.7903225806451613\n",
      "-----------------------------------------------\n",
      "neg_sample_4.txt\n",
      "Load existing positive & negative sample files.\n",
      "Training time: 190.1522\n",
      "Accuracy: 79.26\n",
      "AUROC: 0.8566\n",
      "TPR: 0.7832204273058885\n",
      "TNR: 0.801980198019802\n",
      "PRE: 0.798194370685077\n",
      "F1: 0.7906365071015257\n",
      "(15346, 500)\n",
      "(16946, 500)\n",
      "--------------------finish---------------------------\n"
     ]
    }
   ],
   "source": [
    "opt_semi='on'\n",
    "if opt_semi=='on':\n",
    "    u_N=50000\n",
    "    thr_epoch=5\n",
    "if opt_semi=='on':\n",
    "    unlabel_file='10times.npy'\n",
    "    if name_model[:2]=='m2':\n",
    "        unlabel_data=M1.predict(load_unlabel(config,unlabel_file))\n",
    "    elif name_model[:2]=='m1':\n",
    "        unlabel_data=load_unlabel(config,unlabel_file)\n",
    "    unlabel_len=len(unlabel_data)\n",
    "    u_index = np.arange(unlabel_len)\n",
    "top_pos_list=[1000]\n",
    "top_neg_list=[400,600]\n",
    "for top_pos in top_pos_list:\n",
    "    tmp_test=name_test\n",
    "    for top_neg in top_neg_list:\n",
    "        name_model='m2' \n",
    "        name_model+='_semi_'+'_'.join([str(top_pos),str(top_neg)])\n",
    "        l_model_filename=checkpoint_path+'_'.join([N_label,name_model,name_test])\n",
    "        print(l_model_filename)\n",
    "        acc_list,auroc_list,TPR_list,TNR_list,PRE_list,F1_list=[],[],[],[],[],[]\n",
    "        if opt_semi=='on':\n",
    "            repmat_pos=np.tile(np.array([0,1]),(top_pos,1))\n",
    "            repmat_neg=np.tile(np.array([1,0]),(top_neg,1))\n",
    "        print('MODEL: '+name_model[:2])\n",
    "        print('Semi-supervised mode: '+opt_semi)\n",
    "        print('N_label: '+ N_label)\n",
    "\n",
    "        for neg in neg_file_list: \n",
    "            print('-----------------------------------------------')   \n",
    "            config.Dataset['neg_filename']='neg_sample_'+neg+'.txt'\n",
    "            print(config.Dataset['neg_filename'])\n",
    "            load_pos_neg_samples(config,DTI,verbose=0)\n",
    "            train_x,train_y,test_x,test_y=load_train_test(config,DTI,verbose=opt_verbose_dataset)\n",
    "\n",
    "            if name_model[:2]=='m2':\n",
    "                label_train_x,label_train_y = load_label_train_test(train_x,train_y,N_label=N_label,stacked_model=M1)\n",
    "                label_test_x=M1.predict(test_x)\n",
    "                label_test_y=test_y\n",
    "                \n",
    "            elif name_model[:2]=='m1':\n",
    "                label_train_x,label_train_y = load_label_train_test(train_x,train_y,N_label=N_label,stacked_model=None)\n",
    "                label_test_x=test_x\n",
    "                label_test_y=test_y\n",
    "\n",
    "            input_dim=int(np.shape(label_train_x)[1])\n",
    "\n",
    "            reg=regularizers.l2(0.001)\n",
    "            classifier_layers = [\n",
    "                create_dense_layers(stage='classifier_1', units=256, \n",
    "                                    activation='relu', dropout=0.25,kernel_regularizer=reg),\n",
    "                create_dense_layers(stage='classifier_2', units=128, \n",
    "                                    activation='relu', dropout=0.25,kernel_regularizer=reg),\n",
    "                create_dense_layers(stage='classifier_3', units=64, \n",
    "                                    activation='relu', dropout=0.25,kernel_regularizer=reg),\n",
    "                Dense(2, name='classifier_end'),\n",
    "                Activation('softmax', name='y_predicted')\n",
    "            ]\n",
    "\n",
    "            x_in =Input(shape=(input_dim,), name='x_Input')\n",
    "            y_output=inst_layers(classifier_layers,x_in)\n",
    "            optimizer_l = Adam(lr=label_lr)\n",
    "\n",
    "            classifier=Model(x_in,y_output)\n",
    "            classifier.compile(optimizer=optimizer_l,\n",
    "                        loss={'y_predicted':'categorical_crossentropy'},\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            history_train = []\n",
    "            history_test = []\n",
    "            history_test_F1=[]\n",
    "\n",
    "            checkpoint_cls=0.1\n",
    "            semi_train_x=label_train_x\n",
    "            semi_train_y=label_train_y\n",
    "            if opt_semi=='on':\n",
    "                semi_unlabel=unlabel_data\n",
    "            start=time.time()\n",
    "            for epoch in range(epochs):\n",
    "                index = np.arange(len(semi_train_x))\n",
    "                np.random.shuffle(index)\n",
    "                batches = len(semi_train_x) // batch_size\n",
    "\n",
    "                for i in range(batches):\n",
    "                    index_range =  index[i * batch_size:(i+1) * batch_size]\n",
    "                    loss = classifier.train_on_batch(\n",
    "                        x=[semi_train_x[index_range]],\n",
    "                        y={'y_predicted': semi_train_y[index_range]})\n",
    "                \n",
    "                l = classifier.evaluate(\n",
    "                    x=[semi_train_x],\n",
    "                    y={'y_predicted': semi_train_y},\n",
    "                    verbose=opt_verbose_training)\n",
    "                l_test = classifier.evaluate(\n",
    "                    x=[label_test_x],\n",
    "                    y={'y_predicted': label_test_y},\n",
    "                    verbose=opt_verbose_training)\n",
    "                l_test_F1=[float(get_metrics_values(classifier, label_test_x, label_test_y,verbose=0)[6])]\n",
    "                \n",
    "                if opt_semi=='on' and epoch>=thr_epoch:\n",
    "                    semi_train_x,semi_train_y,semi_unlabel,u_index=loop_self_training(\n",
    "                                                                                classifier,\n",
    "                                                                                label_train_x,label_train_y,\n",
    "                                                                                semi_unlabel,u_index, u_N,\n",
    "                                                                                top_pos,top_neg,\n",
    "                                                                                repmat_pos,repmat_neg)\n",
    "                if opt_save_model=='on' and l_test[opt_acc_loc]>checkpoint_cls:\n",
    "                    checkpoint_cls=l_test[opt_acc_loc]\n",
    "                    classifier.save(l_model_filename+'.h5')\n",
    "                history_train.append(l) \n",
    "                history_test.append(l_test)\n",
    "                history_test_F1.append(l_test_F1)\n",
    "\n",
    "            end=time.time()\n",
    "            print('Training time: %.4f' %(end-start))\n",
    "            if opt_save_model=='on':\n",
    "                classifier.load_weights(l_model_filename+'.h5')\n",
    "\n",
    "            y_pred,accuracy,auroc,TPR,TNR,PRE,F1=get_metrics_values(classifier, label_test_x, label_test_y,\n",
    "                                                                    verbose=1)\n",
    "            acc_list.append(accuracy)\n",
    "            auroc_list.append(auroc)\n",
    "            TPR_list.append(TPR)\n",
    "            TNR_list.append(TNR)\n",
    "            PRE_list.append(PRE)\n",
    "            F1_list.append(F1)\n",
    "\n",
    "            #     plot_roc_curve(classifier,label_test_x,label_test_y)\n",
    "#             plot_epoch_acc_loss(np.concatenate((history_train,history_test),axis=1),validation='on')\n",
    "#             plot_epoch(history_test_F1,loc=0,label='F1')\n",
    "            #     plot_epoch(history_test,loss_loc=opt_loss_loc,label='test_loss')\n",
    "            #   plot_epoch_loss_w_test(history_train,history_test) \n",
    "        \n",
    "        print(label_train_x.shape)\n",
    "        print(semi_train_x.shape)\n",
    "        with open(l_model_filename+'.txt','w') as f:\n",
    "            acc=[float(v) for v in acc_list]\n",
    "            auroc=[float(v) for v in auroc_list]\n",
    "            f1=[float(v) for v in F1_list]\n",
    "            f.write('Avg acc:\\n' + str(np.average(acc)))\n",
    "            f.write('\\nStd acc:\\n' + str(np.std(acc)))\n",
    "            f.write('\\nAvg auroc:\\n' + str(np.average(auroc)))\n",
    "            f.write('\\nStd auroc:\\n' + str(np.std(auroc)))\n",
    "            f.write('\\nAvg F1:\\n' + str(np.average(f1)))\n",
    "            f.write('\\nStd F1:\\n' + str(np.std(f1)))\n",
    "            f.write('\\nacc\\n'+'\\n'.join(acc_list))\n",
    "            f.write('\\nauroc\\n'+'\\n'.join(auroc_list))\n",
    "            f.write('\\nTPR\\n'+'\\n'.join(TPR_list))\n",
    "            f.write('\\nTNR\\n'+'\\n'.join(TNR_list))\n",
    "            f.write('\\nPRE\\n'+'\\n'.join(PRE_list))\n",
    "            f.write('\\nF1\\n'+'\\n'.join(F1_list))\n",
    "\n",
    "        print('--------------------finish---------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------finish---------------------------\n"
     ]
    }
   ],
   "source": [
    "        print('--------------------finish---------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
