{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론에서는 복잡한 함수를 표현할 수 있지만 가중치 설정 작업은 여전히 사람이 수동으로 한다.<br>\n",
    "신경망, neural network 는 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 가진다.<br>\n",
    "ch3 에서는 신경망이 데이터가 무엇인지 식별하는 처리 과정에 대해 알아본다.<br><br>\n",
    "일반적으로 단순 퍼셉트론은 단층 네트워크에서 계단 함수를 활성 함수로 사용한 모델을 가르키고, 다층 퍼셉트론은 신경망을 가르킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step function\n",
    "def step_function(x):\n",
    "    ## general version\n",
    "    #if x>0: return 1\n",
    "    #else : return 0 \n",
    "    ## numpy array support version\n",
    "    y= x>0\n",
    "    return y.astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([-1.,1.,2.])\n",
    "print(step_function(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=np.arange(-5.0,5.0,0.1)\n",
    "y=step_function(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26894142,  0.73105858,  0.88079708])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([-1.,1.,2.])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.arange(-5.,5.,0.1)\n",
    "y=sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변한다. <br>\n",
    "반면, 계단 함수는 0을 경계로 출력이 갑자기 바뀐다. <br>\n",
    "시그모이드의 이 매끈함이 신경망 학습에서 아주 중요한 역할을 하게 된다. <br>\n",
    "신경망에서 시그모이드 함수로 뉴런 사이에 연속적인 실수가 흐를 수 있게 한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공통점으로 크게 보면 두 함수는 비슷한 모양, 입력이 작을 때 출력이 0에 가깝고 클때 출력이 1에 가까운 구조를 가진다. <br>\n",
    "즉, 입력이 중요하면 큰값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. 또한 출력은 항상 0에서 1사이 이다. <br>\n",
    "중요한 공통점으로 둘다 비선형 함수라는 것이다. 선형 함수 사용시 신경망의 층을 깊게 하는 의미가 없어진다. <br>\n",
    "예로 활성 함수 h(x)=cx 라 생각해보면 3층 이어도 y(x)=h(h(h(x)))=c^3*x 가 되어 무의미하게 된다. <br>\n",
    "따라서 선형 함수를 이용해서는 다층의 이점을 살릴 수 없으므로 다층의 혜택을 얻고 싶다면 활성함수로 필히 비선형함수를 사용해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ReLU function\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최근에 자주 사용되는 활성 함수. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "넘파이의 다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있다. \n",
    "<br>특히 np.dot 함수는 굉장히 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# understanding of 3-layer NN\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1']=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1']=np.array([0.1,0.2,0.3])\n",
    "    network['W2']=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2']=np.array([0.1,0.2])\n",
    "    network['W3']=np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3']=np.array([0.1,0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network,x):\n",
    "    W1,W2,W3=network['W1'],network['W2'],network['W3']\n",
    "    b1,b2,b3=network['b1'],network['b2'],network['b3']\n",
    "    \n",
    "    a1=np.dot(x,W1)+b1\n",
    "    z1=sigmoid(a1)\n",
    "    a2=np.dot(z1,W2)+b2\n",
    "    z2=sigmoid(a2)\n",
    "    a3=np.dot(z2,W3)+b3\n",
    "    y=identity_function(a3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31682708  0.69627909]\n"
     ]
    }
   ],
   "source": [
    "network=init_network()\n",
    "x=np.array([1.0,0.5])\n",
    "y=forward(network,x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층 설계<br>\n",
    "신경망은 분류와 회귀 모두 이용할 수 있으며 어떤 문제냐에 따라 출력층에서 사용하는 활성 함수가 달라진다.<br>\n",
    "일반적으로 회귀엔 항등 함수, 분류엔 소프트 맥스 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01821127  0.24519181  0.73659691]\n"
     ]
    }
   ],
   "source": [
    "# softmax function\n",
    "def softmax(a):\n",
    "    exp_a=np.exp(a)\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    return y\n",
    "a=np.array([0.3,2.9,4.0])\n",
    "y=softmax(a)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 컴퓨터가 유한한 크기의 데이터를 다룬다는 특성 상 오버플로우 문제가 발생한다. 즉 너무 큰 값은 표현할 수 없다.<br> \n",
    "가령 e1000은 무한대이며 이런 큰값끼리 나눗셈 시 결과 수치가 '불안정'해진다.<br>\n",
    "이런 문제를 해결하고자 소프트맥스의 지수 함수 계산시 임의의 정수를 더하거나 빼도 결과는 바뀌지 않는다는 성질을 이용한다. <br>\n",
    "임의의 정수로 일반적인 것은 입력 신호 중 최댓값을 이용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax function preventing overflow\n",
    "def softmax(a):\n",
    "    max_a=np.max(a)\n",
    "    exp_a=np.exp(a-max_a)\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01821127  0.24519181  0.73659691]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0.3,2.9,4.0])\n",
    "y=softmax(a)\n",
    "print(y)\n",
    "print(np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더하면 1이 되는 성질 때문에 소프트맥스 함수의 출력을 확률로 해석할 수 있다.<br>\n",
    "즉 위의 결과는 74% 확률로 2번째 클래스, 25%의 확률로 1번째 클래스, 1%의 확률로 0번째 클래스라 얘기할 수 있다. <br>\n",
    "소프트맥스 함수 적용시 각 원소의 대소 관계는 변하지 않는다. 이는 exp함수가 단조 증가 함수이기 때문이다. <br>\n",
    "따라서 신경망을 이용한 classification 문제에서 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않는다. 즉 신경망으로 분류시 출력층의 소프트맥스 함수를 생략해도 된다. <br>\n",
    "현업에서도 지수 함수 계산에 드는 자원낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는것이 일반적이다. <br>\n",
    "추론 단계에서는 출력층의 소프트맥스 함수 생략하는 것이 일반적, 한편 학습 단계에서는 출력층에서 소프트맥스 함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MNIST 문제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-496202a326db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
