{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd()+'\\simple-examples')\n",
    "import reader\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell, DropoutWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    init_scale=0\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    num_steps=20\n",
    "    hidden_size=200\n",
    "    max_epoch=4\n",
    "    max_max_epoch=13\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.5\n",
    "    batch_size=20\n",
    "    vocab_size=10000\n",
    "\n",
    "config=SmallConfig()\n",
    "eval_config=SmallConfig()\n",
    "eval_config.batch_size=1\n",
    "eval_config.num_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self,config,is_training=False):\n",
    "        self.batch_size=config.batch_size\n",
    "        self.num_steps=config.num_steps\n",
    "        input_size=[config.batch_size,config.num_steps]\n",
    "        self.input_data=tf.placeholder(tf.int32,input_size)\n",
    "        self.targets=tf.placeholder(tf.int32,input_size)\n",
    "        \n",
    "        lstm_cell=BasicLSTMCell(config.hidden_size,forget_bias=0.0,state_is_tuple=True)\n",
    "        \n",
    "        if is_training and config.keep_prob<1:\n",
    "            lstm_cell=DropoutWrapper(lstm_cell,config.keep_prob)\n",
    "        \n",
    "        cell = MultiRNNCell([lstm_cell]*config.num_layers,state_is_tuple=True)\n",
    "        self.initial_state=cell.zero_state(config.batch_size,tf.float32)\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding_size=[config.vocab_size,config.hidden_size]\n",
    "            embedding=tf.get_variable(\"embedding\",embedding_size)\n",
    "            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\n",
    "        \n",
    "        if is_training and config.keep_prob<1:\n",
    "            inputs=tf.nn.dropout(inputs,config.keep_prob)\n",
    "        \n",
    "        outputs=[]\n",
    "        state=self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(config.num_steps):\n",
    "                if time_step>0:tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output,state)=cell(inputs[:,time_step,:],state)\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        output=tf.reshape(tf.concat(outputs,1),[-1,config.hidden_size])\n",
    "        softmax_w_size=[config.hidden_size,config.vocab_size]\n",
    "        softmax_w=tf.get_variable(\"softmax_w\",softmax_w_size)\n",
    "        softmax_b=tf.get_variable(\"sorfmax_b\",[config.vocab_size])\n",
    "        logits=tf.matmul(output,softmax_w)+softmax_b\n",
    "        \n",
    "        loss=tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],[tf.reshape(self.targets,[-1])],[tf.ones([config.batch_size*config.num_steps])])\n",
    "        \n",
    "        self.cost=tf.reduce_sum(loss)/config.batch_size\n",
    "        self.final_state=state\n",
    "        \n",
    "        if not is_training:return\n",
    "        \n",
    "        self.lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        \n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(self.cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op=optimizer.apply_gradients(zip(grads,tvars))\n",
    "        \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr,lr_value))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session,m,data,is_training=False):\n",
    "    epoch_size=((len(data)//m.batch_size)-1)\n",
    "    start_time=time.time()\n",
    "    costs=0.0\n",
    "    iters=0\n",
    "    \n",
    "    eval_op=m.train_op if is_training else tf.no_op()\n",
    "    \n",
    "    state_list=[]\n",
    "    for c,h in m.initial_state:\n",
    "        state_list.extend([c.eval(),h.eval()])\n",
    "    \n",
    "    ptb_iter=reader.ptb_iterator(data,m.batch_size,m.num_steps)\n",
    "#     x_,y_=tf.unstack(ptb_iter)\n",
    "#         for step in range(m.epoch_size):\n",
    "#     step=0\n",
    "    for step,(x,y) in enumerate(ptb_iter):\n",
    "        fetch_list=[m.cost]\n",
    "        for c,h in m.final_state:\n",
    "            fetch_list.extend([c,h])\n",
    "        fetch_list.append(eval_op)\n",
    "\n",
    "        feed_dict={m.input_data:x,m.targets:y}\n",
    "        for i in range(len(m.initial_state)):\n",
    "            c,h = m.initial_state[i]\n",
    "            feed_dict[c],feed_dict[h]=state_list[i*2:(i+1)*2]\n",
    "\n",
    "        cost,*state_list,_=session.run(fetch_list,feed_dict)\n",
    "\n",
    "        cost+=cost\n",
    "        iters+=m.num_steps\n",
    "\n",
    "        if is_training and step%(epoch_size//10)==10:\n",
    "            print(\"%.3f perplexity: %.3f spped: %.0f wps\" % (\n",
    "            step*1.0/epoch_size,np.exp(costs/iters),iters*m.batch_size/(time.time()-start_time)))\n",
    "#         step+=1\n",
    "    return np.exp(costs/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=reader.ptb_raw_data('./simple-examples/data')\n",
    "train_data,valid_data,test_data,_=raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-02608a1b26b8>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.000 perplexity: 1.000 spped: 2010 wps\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(),tf.Session() as session:\n",
    "#     tf.enable_eager_execution()\n",
    "    \n",
    "    initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "    with tf.variable_scope(\"model\",reuse=None,initializer=initializer):\n",
    "        m=PTBModel(config,is_training=True)\n",
    "    with tf.variable_scope(\"model\",reuse=True,initializer=initializer):\n",
    "        mvalid=PTBModel(config)\n",
    "        mtest=PTBModel(eval_config)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(config.max_max_epoch):\n",
    "        lr_decay=config.lr_decay ** max(i-config.max_epoch,0.0)\n",
    "        m.assign_lr(session,config.learning_rate*lr_decay)\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i+1,session.run(m.lr)))\n",
    "        \n",
    "        perplexity=run_epoch(session,m,train_data,is_training=True)\n",
    "        print(\"Epoch: %d Training Perplexity: %.3f\" % (i+1,perplexity))\n",
    "        \n",
    "        perplexity=run_epoch(session,mvalid,valid_data)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i+1,perplexity))\n",
    "        \n",
    "    perplexity=run_epoch(session,mtest,test_data)\n",
    "    print(\"Test Perplexity: %.3f\" % perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
